{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LishaRamon/applied-ml/blob/main/HW3_Comparison_of_ML_Classification_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcS6xH5PlrX7"
      },
      "source": [
        "# HW3: - Classification Comparison with Synthetic Data\n",
        "\n",
        "Thought process:\n",
        "\n",
        "1. Need to compare **6 classifiers** on **4 datasets**\n",
        "3. For every dataset need to: split data → fit on train → evaluate on train and test\n",
        "4. Also need to visualize the decision boundaries\n",
        "\n",
        "The 6 classifiers are:\n",
        "- Naive Bayes\n",
        "- Logistic Regression\n",
        "- Quadratic Discriminant Analysis(QDA)\n",
        "- SVM with radial basis functions (RBF) kernel\n",
        "- Decision Tree\n",
        "- KNN with K=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbSsxSgRlrYC"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 1: Importing libraries\n",
        "\n",
        "- `numpy` and `matplotlib`\n",
        "- `sklearn.datasets` for creating synthetic data\n",
        "- `sklearn.model_selection` for train/test splitting\n",
        "- All 6 classifier classes\n",
        "- `classification_report` for evaluation\n",
        "\n",
        "Also set a random seed so results are reproducible during every run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-G1BTRkxlrYD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# data generating\n",
        "from sklearn.datasets import make_blobs, make_circles, make_moons\n",
        "\n",
        "# split data into train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# import 6 classifers\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# for evaluation\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# setting random seed for consistent reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtis1mF4lrYF"
      },
      "source": [
        "## Step 2: Define Classifers\n",
        "\n",
        "Each dataset needs a fresh set of classifiers so nothing carries over from a previous run. I'll wrap them in a function and call it once per dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_yjf_D8MlrYF"
      },
      "outputs": [],
      "source": [
        "def get_classifiers():\n",
        "    #returns a fresh dict for all 6 classifiers\n",
        "    return {\n",
        "        'Naive Bayes':    GaussianNB(),\n",
        "        'Logistic Reg':   LogisticRegression(max_iter=1000, random_state=SEED),\n",
        "        'QDA':            QuadraticDiscriminantAnalysis(),\n",
        "        'SVM (RBF)':      SVC(kernel='rbf', random_state=SEED),\n",
        "        'Decision Tree':  DecisionTreeClassifier(random_state=SEED),\n",
        "        'KNN (K=1)':      KNeighborsClassifier(n_neighbors=1),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Decision Boundary Plot Function\n",
        "\n",
        "Since the data is 2D, I can visualize each classifier's decision boundary by predicting the class for every point on a mesh grid, coloring the regions, then overlaying the actual data points on top"
      ],
      "metadata": {
        "id": "qwfu3unAZ0o3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_DAD-4-DaYwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JF1VyDhVlrYG"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundaries(X, y, fitted_classifiers, dataset_name):\n",
        "    \"\"\" Plots the decision boundary for each fitted classifier over the 2D data\n",
        "    Param:\n",
        "        x: feature array\n",
        "        y: label array\n",
        "        fitted_classifiers: dict of model type\n",
        "        dataset_name: plot title\"\"\"\n",
        "\n",
        "    # decision region(bkgd) colors + data points\n",
        "    cmap_bg     = ListedColormap(['#FFAAAA', '#AAAAFF'])  # light red/blue regions\n",
        "    cmap_points = ListedColormap(['#CC0000', '#0000CC'])  # dark red/blue dots\n",
        "\n",
        "    # mesh grid over feature space\n",
        "    h = 0.05  # step size (smaller = finer grid;slower to render)\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(\n",
        "        np.arange(x_min, x_max, h),\n",
        "        np.arange(y_min, y_max, h)\n",
        "    )\n",
        "\n",
        "    # 1 subplot per classifier= shape( 2 rows x 3 columns)\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(16, 9))\n",
        "    axes = axes.flatten()\n",
        "    fig.suptitle(f'Decision Boundaries — {dataset_name}', fontsize=14, fontweight='bold')\n",
        "\n",
        "#loop picks next subplot+classifer together\n",
        "    for ax, (name, clf) in zip(axes, fitted_classifiers.items()): #\n",
        "        # predict class for every grid point — fills in the colored regions\n",
        "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) #asks classifier to predict class for each point and flatten the 2D grid into a long list of x and y coordinates\n",
        "        Z = Z.reshape(xx.shape) #puts predictions into 2D grid shapefor color\n",
        "\n",
        "        # fill decision area w color\n",
        "        ax.contourf(xx, yy, Z, cmap=cmap_bg, alpha=0.5)\n",
        "\n",
        "        # plot points on top\n",
        "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_points,\n",
        "                   edgecolors='k', s=25, linewidth=0.4)\n",
        "\n",
        "        ax.set_title(name, fontsize=11)\n",
        "        ax.set_xlim(xx.min(), xx.max())\n",
        "        ax.set_ylim(yy.min(), yy.max())\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}